{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13716158,"sourceType":"datasetVersion","datasetId":8726016},{"sourceId":13755447,"sourceType":"datasetVersion","datasetId":8753086},{"sourceId":13755758,"sourceType":"datasetVersion","datasetId":8753310}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ===========================================\n#  Gujarati Next Word Prediction using Bi-LSTM\n#  (Based on \"Next Word Prediction in Hindi Using Deep Learning Techniques\")\n# ===========================================\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport re\nimport pickle\nimport matplotlib.pyplot as plt\nimport os","metadata":{"id":"fEw0-yJo6MiS","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:22:22.809275Z","iopub.execute_input":"2025-11-16T15:22:22.809830Z","iopub.status.idle":"2025-11-16T15:22:22.814996Z","shell.execute_reply.started":"2025-11-16T15:22:22.809805Z","shell.execute_reply":"2025-11-16T15:22:22.814168Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# --- GPU setup ---\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"âœ… Enabled memory growth for {len(gpus)} GPU(s).\")\n    except RuntimeError as e:\n        print(f\"âš ï¸ GPU Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:56:32.993833Z","iopub.execute_input":"2025-11-16T15:56:32.994205Z","iopub.status.idle":"2025-11-16T15:56:32.999455Z","shell.execute_reply.started":"2025-11-16T15:56:32.994183Z","shell.execute_reply":"2025-11-16T15:56:32.998615Z"}},"outputs":[{"name":"stdout","text":"âœ… Enabled memory growth for 2 GPU(s).\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# --- Hyperparameters for Data ---\nDATA_PATH = \"/kaggle/input/gu-processed/gu_processed.txt\"\nNUM_LINES = 50000 \nVOCAB_TARGET = 30000  # <--- UPDATED: Limit vocab to top 30k words\nSEQ_LENGTH = 10       # <--- UPDATED: Increased sequence length for more context\n\nprint(f\"Loading {NUM_LINES} lines from dataset...\")\nlines = []\nwith open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n    for i, line in enumerate(f):\n        if i >= NUM_LINES:\n            break\n        line = line.strip()\n        if line:\n            lines.append(line)\n\nprint(f\"âœ… Loaded {len(lines)} lines.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:54:24.203940Z","iopub.execute_input":"2025-11-16T15:54:24.204580Z","iopub.status.idle":"2025-11-16T15:54:24.304497Z","shell.execute_reply.started":"2025-11-16T15:54:24.204558Z","shell.execute_reply":"2025-11-16T15:54:24.303736Z"}},"outputs":[{"name":"stdout","text":"Loading 50000 lines from dataset...\nâœ… Loaded 25000 lines.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# --- Text Cleaning ---\ndef clean_gujarati_text(text):\n    text = re.sub(r'[^\\u0A80-\\u0AFF\\s]', '', text)  # keep Gujarati chars\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ncleaned_text = ' '.join(clean_gujarati_text(line) for line in lines)\nprint(f\"Sample cleaned text: {cleaned_text[:150]}...\")\n\n# --- Tokenization ---\n# <--- UPDATED: Apply vocab limit and OOV token\ntokenizer = Tokenizer(num_words=VOCAB_TARGET, oov_token=\"<UNK>\") \ntokenizer.fit_on_texts([cleaned_text])\n\n# Save tokenizer\nwith open(\"gujarati_tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)\n\n# <--- UPDATED: The vocab_size for the model is now our target\nvocab_size = VOCAB_TARGET \nprint(f\"âœ… Vocabulary Size set to: {vocab_size}\")\n\n# --- Create sequences (10:1 ratio) ---\ntokens = tokenizer.texts_to_sequences([cleaned_text])[0]\nsequences = []\n\n# <--- UPDATED: Loop will use new SEQ_LENGTH\nfor i in range(SEQ_LENGTH, len(tokens)): \n    seq = tokens[i-SEQ_LENGTH:i+1]\n    sequences.append(seq)\n\nsequences = np.array(sequences)\nX, y = sequences[:, :-1], sequences[:, -1]\n\nprint(f\"Total sequences: {len(X)}\")\nprint(f\"Input shape: {X.shape}, Output shape: {y.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:54:38.174454Z","iopub.execute_input":"2025-11-16T15:54:38.175071Z","iopub.status.idle":"2025-11-16T15:54:44.439790Z","shell.execute_reply.started":"2025-11-16T15:54:38.175049Z","shell.execute_reply":"2025-11-16T15:54:44.438958Z"}},"outputs":[{"name":"stdout","text":"Sample cleaned text: àª† àªµà«€àª¡àª¿àª¯à«‹ àªœà«àª“ àªŠàª‚àªàª¾ àª®àª¾àª°à«àª•à«‡àªŸàª¯àª¾àª°à«àª¡ àª†àªœàª¥à«€ àªœà«àª²àª¾àªˆ àª¸à«àª§à«€ àª¬àª‚àª§ àª®àª¿àª¥à«‡àª¨à«‹àª² àª†àªµà«àª¯à«‹ àª•à«àª¯àª¾àª‚àª¥à«€ àª†àª–àª°à«‡ àª¤à«àª°àª£ àª°àª¾àªœà«àª¯à«‹àª®àª¾àª‚ àª®àª³à«‡àª² àª¹àª¾àª° àªªàª° àª•à«‹àª‚àª—à«àª°à«‡àª¸ àª…àª§à«àª¯àª•à«àª· àª°àª¾àª¹à«àª² àª—àª¾àª‚àª§à«€ àª¦à«àªµàª¾àª°àª¾ àªªà«àª°àª¥àª® àªªà«àª°...\nâœ… Vocabulary Size set to: 30000\nTotal sequences: 1020578\nInput shape: (1020578, 10), Output shape: (1020578,)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# --- Create Train/Test Split ---\n# <--- NEW: Added train/test split, which model.fit() needs\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Train shapes: {X_train.shape}, {y_train.shape}\")\nprint(f\"Test shapes: {X_test.shape}, {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:54:55.200558Z","iopub.execute_input":"2025-11-16T15:54:55.201158Z","iopub.status.idle":"2025-11-16T15:54:55.351183Z","shell.execute_reply.started":"2025-11-16T15:54:55.201133Z","shell.execute_reply":"2025-11-16T15:54:55.350417Z"}},"outputs":[{"name":"stdout","text":"Train shapes: (816462, 10), (816462,)\nTest shapes: (204116, 10), (204116,)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# --- Tuned Model Hyperparameters ---\n# These must match the values from Part 1\n# vocab_size = 30000 \n# SEQ_LENGTH = 10\n\nEMBED_DIM = 250       # <--- UPDATED: Increased embedding dim\nLSTM_UNITS = 256      # <--- UPDATED: Increased LSTM capacity\nDENSE_UNITS = 512     # <--- UPDATED: Increased Dense layer capacity\nLR = 0.001            # <--- UPDATED: Slightly higher start LR\nEPOCHS = 100\nBATCH_SIZE = 128      # 128 or 256 is good\n\nmodel = Sequential([\n    Embedding(input_dim=vocab_size, \n              output_dim=EMBED_DIM, \n              input_length=SEQ_LENGTH),\n    \n    # <--- UPDATED: Added internal dropout\n    Bidirectional(LSTM(LSTM_UNITS, \n                       return_sequences=True,\n                       dropout=0.2,            \n                       recurrent_dropout=0.2)),\n    \n    # <--- UPDATED: Removed separate Dropout, added internal dropout\n    Bidirectional(LSTM(LSTM_UNITS,\n                       dropout=0.2, \n                       recurrent_dropout=0.2)),\n    \n    Dense(DENSE_UNITS, activation='relu'),\n    Dropout(0.5),  # <--- UPDATED: Stronger dropout before final layer\n    \n    Dense(vocab_size, activation='softmax')\n])\n\noptimizer = Adam(learning_rate=LR)\nmodel.compile(loss='sparse_categorical_crossentropy', \n              optimizer=optimizer, \n              metrics=['accuracy'])\nmodel.build(input_shape=(None, SEQ_LENGTH))\nmodel.summary()\n\n# --- Callbacks ---\nearly_stopper = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n# <--- UPDATED: Lowered min_lr\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, min_lr=1e-6) \n\nprint(\"\\nğŸš€ Starting Tuned Bi-LSTM Training...\\n\")\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=[early_stopper, reduce_lr],\n    verbose=1\n)\n\n# --- Evaluate ---\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\nprint(f\"\\nğŸ¯ Final Test Accuracy: {test_acc:.4f}\")\nprint(f\"ğŸ§¾ Final Test Loss: {test_loss:.4f}\")\n\n# --- Save Model ---\nmodel.save(\"bilstm_gujarati_tuned_model.keras\")\nprint(\"ğŸ’¾ Saved: bilstm_gujarati_tuned_model.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:56:38.744229Z","iopub.execute_input":"2025-11-16T15:56:38.744555Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_13\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding_13 (\u001b[38;5;33mEmbedding\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m250\u001b[0m)        â”‚     \u001b[38;5;34m7,500,000\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_16                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m512\u001b[0m)        â”‚     \u001b[38;5;34m1,038,336\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)                 â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_17                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚     \u001b[38;5;34m1,574,912\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)                 â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_16 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚       \u001b[38;5;34m262,656\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_17 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30000\u001b[0m)          â”‚    \u001b[38;5;34m15,390,000\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ embedding_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,500,000</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_16                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,038,336</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_17                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)          â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">15,390,000</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,765,904\u001b[0m (98.29 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,765,904</span> (98.29 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,765,904\u001b[0m (98.29 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,765,904</span> (98.29 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\nğŸš€ Starting Tuned Bi-LSTM Training...\n\nEpoch 1/100\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m786s\u001b[0m 121ms/step - accuracy: 0.1203 - loss: 7.6329 - val_accuracy: 0.1428 - val_loss: 7.1239 - learning_rate: 0.0010\nEpoch 2/100\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m769s\u001b[0m 121ms/step - accuracy: 0.1450 - loss: 7.0320 - val_accuracy: 0.1526 - val_loss: 6.9115 - learning_rate: 0.0010\nEpoch 3/100\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m771s\u001b[0m 121ms/step - accuracy: 0.1558 - loss: 6.7354 - val_accuracy: 0.1600 - val_loss: 6.7971 - learning_rate: 0.0010\nEpoch 4/100\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m774s\u001b[0m 121ms/step - accuracy: 0.1636 - loss: 6.5273 - val_accuracy: 0.1643 - val_loss: 6.7215 - learning_rate: 0.0010\nEpoch 5/100\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m779s\u001b[0m 122ms/step - accuracy: 0.1733 - loss: 6.3396 - val_accuracy: 0.1677 - val_loss: 6.6823 - learning_rate: 0.0010\nEpoch 6/100\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m775s\u001b[0m 122ms/step - accuracy: 0.1796 - loss: 6.1889 - val_accuracy: 0.1712 - val_loss: 6.6703 - learning_rate: 0.0010\nEpoch 7/100\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m777s\u001b[0m 122ms/step - accuracy: 0.1876 - loss: 6.0572 - val_accuracy: 0.1724 - val_loss: 6.6829 - learning_rate: 0.0010\nEpoch 8/100\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.1919 - loss: 5.9489\nEpoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m777s\u001b[0m 122ms/step - accuracy: 0.1919 - loss: 5.9489 - val_accuracy: 0.1738 - val_loss: 6.7174 - learning_rate: 0.0010\nEpoch 9/100\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m777s\u001b[0m 122ms/step - accuracy: 0.1996 - loss: 5.7981 - val_accuracy: 0.1750 - val_loss: 6.7640 - learning_rate: 5.0000e-04\nEpoch 10/100\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.2058 - loss: 5.6941\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m6379/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m784s\u001b[0m 123ms/step - accuracy: 0.2058 - loss: 5.6941 - val_accuracy: 0.1754 - val_loss: 6.8215 - learning_rate: 5.0000e-04\nEpoch 11/100\n\u001b[1m3001/6379\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m6:35\u001b[0m 117ms/step - accuracy: 0.2120 - loss: 5.5911","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# --- Plot Accuracy and Loss ---\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title(\"Gujarati Bi-LSTM Accuracy (Paper Architecture)\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title(\"Gujarati Bi-LSTM Loss (Paper Architecture)\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}